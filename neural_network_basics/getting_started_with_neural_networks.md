# ç¥ç»ç½‘ç»œå…¥é—¨

+ ç¥ç»ç½‘ç»œä¸å¤šå±‚æ„ŸçŸ¥æœºï¼šåŸºç¡€çŸ¥è¯†ï¼Œæ¿€æ´»å‡½æ•°ã€åå‘ä¼ æ’­ã€æŸå¤±å‡½æ•°ã€æƒå€¼åˆå§‹åŒ–å’Œæ­£åˆ™åŒ–
+ å·ç§¯ç¥ç»ç½‘ç»œï¼šç»Ÿæ²»å›¾åƒé¢†åŸŸçš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œå‘å±•å†å²ã€å·ç§¯æ“ä½œå’Œæ± åŒ–æ“ä½œ
+ å¾ªç¯ç¥ç»ç½‘ç»œï¼šç»Ÿæ²»åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œç»“æ„ï¼ŒRNNã€GRUå’ŒLSTM

å­¦ä¹ èµ„æºæ¨è

1. å´æ©è¾¾çš„æ·±åº¦å­¦ä¹ è¯¾ç¨‹: https://www.deeplearning.ai
2. æå®æ¯…çš„æ·±åº¦å­¦ä¹ è¯¾ç¨‹
3. ã€Šdeeplearningã€‹ä¿—ç§°èŠ±ä¹¦: https://github.com/exacity/deeplearningbook-chinese
4. å‘¨å¿—åçš„ã€Šæœºå™¨å­¦ä¹ ã€‹

## äººå·¥ç¥ç»å…ƒ

Artificial Neural Unit

äººå·¥ç¥ç»å…ƒï¼š äººç±»ç¥ç»å…ƒä¸­æŠ½è±¡å‡ºæ¥çš„æ•°å­¦æ¨¡å‹

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230527224207853.png)

1904å¹´ç ”ç©¶å‡ºäººç±»ç¥ç»å…ƒï¼Œå…¶ä¸­ï¼š

+ æ ‘çªï¼šç±»ä¼¼äºinput
+ ç»†èƒæ ¸ï¼šç±»ä¼¼äºoperation ï¼šå¤„ç†æ“ä½œ+æ¿€æ´»å‡½æ•°
+ è½´çªæœ«æ¢¢ï¼šç±»ä¼¼äºoutput

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230527224610266.png)

1943å¹´å¿ƒç†å­¦å®¶W.S. McCullochå’Œæ•°ç†é€»è¾‘å­¦å®¶W.Pittsç ”ç©¶å‡ºäººå·¥ç¥ç»å…ƒ,ç§°ä¸ºM-Ğ æ¨¡å‹ã€‚
$$
f\left(\sum_{i=1}^{N} I_{i} \cdot W_{i}\right)=y
$$
äººå·¥ç¥ç»ç½‘ç»œï¼š å¤§é‡ç¥ç»å…ƒä»¥æŸç§è¿æ¥æ–¹å¼æ„æˆçš„æœºå™¨å­¦ä¹ æ¨¡å‹

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230527224941075.png)

ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œï¼š1958å¹´ï¼Œè®¡ç®—æœºç§‘å­¦å®¶Rosenblattæå‡ºçš„Perceptronï¼ˆæ„ŸçŸ¥æœºï¼‰
$$
o\,=\,\sigma(\langle\mathbf{w},\mathbf{x}\rangle+b)
$$

$$
\sigma(x)=\left\{\begin{array}{ll}
1 & \text { if } x>0 \\
0 & \text { otherwise }
\end{array}\right.
$$

å¼•å‘äº†ç¬¬ä¸€æ³¢ç¥ç»ç½‘ç»œçš„çƒ­æ½®ï¼Œä½†æ„ŸçŸ¥æœºçš„è‡´å‘½ç¼ºç‚¹æ˜¯ï¼šMinskyåœ¨1969å¹´è¯æ˜Perceptronæ— æ³•è§£å†³å¼‚æˆ–é—®é¢˜ã€‚æ ¹æºåœ¨äºï¼ŒäºŒç»´å±‚é¢ä¸Šç¥ç»ç½‘ç»œæ˜¯ä¸€æ¡ç›´çº¿ã€‚æ— æ³•åˆ’åˆ†å¼‚æˆ–çš„åŒºé—´ã€‚
$$
\begin{array}{rlr}
0 & =\sigma\left(\mathrm{x}_{0} \mathrm{w}_{0}+\mathrm{x}_{1} \mathrm{w}_{1}+\mathrm{b}\right) \\
0 & =\mathrm{x}_{0} \mathrm{w}_{0}+\mathrm{x}_{1} \mathrm{w}_{1}+\mathrm{b} \\
\mathrm{x}_{1} \mathrm{w}_{1} & =0-\mathrm{x}_{0} \mathrm{w}_{0}-\mathrm{b} \\
\mathrm{x}_{1} & =  -\frac{\mathrm{w}_{0}}{\mathrm{w}_{1}} \mathrm{x}_{0}-\frac{\mathrm{b}}{\mathrm{w}_{1}} \\
\mathrm{y} & =  \mathrm{kx}+\mathrm{b}
\end{array}
$$
![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230528223959705.png)

## å¤šå±‚æ„ŸçŸ¥æœº

å¤šå±‚æ„ŸçŸ¥æœº(Multi Layer Perceptron,MLP):å•å±‚ç¥ç»ç½‘ç»œåŸºç¡€ä¸Šå¼•å…¥ä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚,ä½¿ç¥ç»ç½‘ç»œæœ‰å¤šä¸ªç½‘ç»œå±‚ï¼Œå› è€Œå¾—åå¤šå±‚æ„ŸçŸ¥æœºã€‚

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230528224416573.png)

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230528224528305.png)

éšè—å±‚çš„è¾“å…¥æƒé‡çŸ©é˜µæ˜¯  $\mathrm{W}_{4 \times 5}$ 

éšè—å±‚çš„æƒé‡çŸ©é˜µæ˜¯  $W_{5 \times 3} $

å‰å‘ä¼ æ’­:ï¼ˆä»¥ä¸‹æ–¹ä¾¿èµ·è§çœç•¥äº†åç½®ï¼‰
$$
\begin{array}{l}
\sigma\left(\mathrm{X}_{1 \times 4} \cdot \mathrm{W}_{\mathrm{h}}\right)=\mathrm{H}_{1 \times 5} \\
\sigma\left(\mathrm{H}_{1 \times 5} \cdot \mathrm{W}_{\mathrm{o} \times 3}\right)=\mathrm{O}_{1 \times 3}
\end{array}
$$
æ•´ä¸ªè¿‡ç¨‹å¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œç½‘ç»œé€€åŒ–ä¸ºå•å±‚ç½‘ç»œï¼Œä¸‹é¢è¯æ˜ï¼š
$$
\begin{aligned}
\boldsymbol{H} & =\boldsymbol{X} \boldsymbol{W}_{\mathrm{h}}+\boldsymbol{b}_{\mathrm{h}} \\
\boldsymbol{O} & =\boldsymbol{H} \boldsymbol{W}_{\mathrm{o}}+\boldsymbol{b}_{\mathrm{o}} \\
\boldsymbol{O} & =\left(\boldsymbol{X} \boldsymbol{W}_{\mathrm{h}}+\boldsymbol{b}_{\mathrm{h}}\right) \boldsymbol{W}_{\mathrm{o}}+\boldsymbol{b}_{\mathrm{o}}=\boldsymbol{X} \boldsymbol{W}_{\mathrm{h}} \boldsymbol{W}_{\mathrm{o}}+\boldsymbol{b}_{\mathrm{h}} \boldsymbol{W}_{\mathrm{o}}+\boldsymbol{b}_{\mathrm{c}}
\end{aligned}
$$
å¯è§ï¼Œæœ€ç»ˆç½‘ç»œè¿˜æ˜¯å¯ä»¥åŒ–ç®€æ›¿ä»£ï¼Œé€€åŒ–ä¸ºXW+bçš„å•å±‚ç½‘ç»œã€‚

å½“éšè—å±‚åŠ å…¥**æ¿€æ´»å‡½æ•°ï¼Œå¯é¿å…ç½‘ç»œé€€åŒ–**
$$
\begin{array}{l}
\mathbf{h}=\sigma\left(\mathbf{W}_{1} \mathbf{x}+\mathbf{b}_{1}\right) \\
\mathbf{0}=\mathbf{w}_{2}^{\mathrm{T}} \mathbf{h}+\mathbf{b}_{2}
\end{array}
$$

## æ¿€æ´»å‡½æ•°

ä½œç”¨ï¼š

1. è®©å¤šå±‚æ„ŸçŸ¥æœºæˆä¸ºçœŸæ­£çš„å¤šå±‚ï¼Œå¦åˆ™ç­‰ä»·äºä¸€å±‚ï¼›
2. å¼•å…¥éçº¿æ€§ï¼Œä½¿ç½‘ç»œå¯ä»¥é€¼è¿‘ä»»æ„éçº¿æ€§å‡½æ•°ï¼ˆè¯¦è§ï¼š**ä¸‡èƒ½é€¼è¿‘å®šç†**ï¼Œuniversal approximatorï¼‰

æ¿€æ´»å‡½æ•°éœ€è¦å…·å¤‡ä»¥ä¸‹å‡ ç‚¹æ€§è´¨:

1. **è¿ç»­å¹¶å¯å¯¼**ï¼ˆå…è®¸å°‘æ•°ç‚¹ä¸Šä¸å¯å¯¼ï¼‰ï¼Œä¾¿äºåˆ©ç”¨æ•°å€¼ä¼˜åŒ–çš„æ–¹æ³•æ¥å­¦ä¹ ç½‘ç»œå‚æ•°
2. æ¿€æ´»å‡½æ•°åŠå…¶å¯¼å‡½æ•°è¦å°½å¯èƒ½çš„ç®€å•ï¼Œæœ‰åˆ©äºæé«˜ç½‘ç»œè®¡ç®—æ•ˆç‡
3. æ¿€æ´»å‡½æ•°çš„å¯¼å‡½æ•°çš„**å€¼åŸŸ**è¦åœ¨**åˆé€‚**åŒºé—´å†…ï¼Œåå‘ä¼ æ’­ä¸­ä¼šä½¿ç”¨åˆ°æ¿€æ´»å‡½æ•°çš„å¯¼å‡½æ•°ï¼Œæ•°å€¼ä¼šå½±å“åˆ°æƒé‡çš„æ›´æ–°ï¼Œä¸èƒ½å¤ªå¤§ä¹Ÿä¸èƒ½å¤ªå°ï¼Œå¦åˆ™ä¼šå½±å“è®­ç»ƒçš„æ•ˆç‡å’Œç¨³å®šæ€§ã€‚

**å¸¸è§æ¿€æ´»å‡½æ•°**ï¼šSigmoidï¼ˆSå‹ï¼‰ ï¼Œ Tanhï¼ˆåŒæ›²æ­£åˆ‡ï¼‰ï¼Œ ReLUï¼ˆä¿®æ­£çº¿æ€§å•å…ƒï¼‰

### Sigmoidï¼ˆSå‹ï¼‰

å¸¸ç”¨äºRNNä»¥åŠäºŒåˆ†ç±»ä¸­ã€‚åšäºŒåˆ†ç±»çš„è¾“å‡ºçš„æ¿€æ´»å‡½æ•°ï¼›åšå¾ªç¯ç¥ç»ç½‘ç»œä¸­é—¨æ§å•å…ƒçš„æ¿€æ´»å‡½æ•°ã€‚

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230528225633959.png)

æœ‰é¥±å’ŒåŒºï¼Œå¯¼å‡½æ•°çš„é¥±å’ŒåŒºï¼ˆç¥ç»å…ƒçš„å€¼å·®ä¸å¤šï¼Œæ¢¯åº¦å·®ä¸å¤šï¼‰ï¼Œæ›´æ–°å›°éš¾ã€‚
$$
\begin{array}{c}
g(z)=\frac{1}{1+e^{-z}} \\
g^{\prime}(z)=g(z) *(1-g(z))
\end{array}
$$
### Tanhï¼ˆåŒæ›²æ­£åˆ‡ï¼‰

ç‰¹ç‚¹ï¼Œé›¶å‡å€¼ã€‚åŒæ ·å­˜åœ¨**é¥±å’ŒåŒº**ï¼Œä¸åˆ©äºæ¢¯åº¦ä¼ æ’­ã€‚ä¸­é—´çš„æ˜¯**çº¿æ€§åŒº**ã€‚

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230528225717490.png)
$$
\begin{array}{c}
\tanh (x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} \\
g^{\prime}(z)=1-(g(z))^{2}
\end{array}
$$
### ReLUï¼ˆä¿®æ­£çº¿æ€§å•å…ƒï¼‰

ä¸å­˜åœ¨é¥±å’ŒåŒºï¼Œå·ç§¯ç¥ç»ç½‘ç»œä¸­ç»å¸¸ä½¿ç”¨ã€‚

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230528225751968.png)
$$
\begin{array}{c}
\text { Relu }=\max (0, x) \\
g^{\prime}(z)=\left\{\begin{array}{ll}
1 & \text { if } \mathrm{z}>0 \\
\text { undefined } & \text { if } \mathrm{z}=0 \\
0 & \text { if } \mathrm{z}<0
\end{array}\right.
\end{array}
$$

## åå‘ä¼ æ’­ï¼ˆBack Propagationï¼‰

### å‰å‘ä¼ æ’­

è¾“å…¥å±‚æ•°æ®ä»å‰å‘åï¼Œæ•°æ®é€æ­¥ä¼ é€’è‡³è¾“å‡ºå±‚
$$
\begin{array}{l}
\mathrm{z}=\mathrm{x} \cdot \mathrm{W}^{(1)} \\
\mathrm{h}=\phi(\mathrm{z}) \\
\mathrm{O}=\mathrm{h} \cdot \mathrm{W}^{(2)}
\end{array}
$$
### åå‘ä¼ æ’­

**æŸå¤±å‡½æ•°**å¼€å§‹ä»åå‘å‰ï¼Œ**æ¢¯åº¦**é€æ­¥ä¼ é€’è‡³ä¸€å±‚

åå‘ä¼ æ’­ä½œç”¨ï¼šç”¨äºæƒé‡æ›´æ–°ï¼Œä½¿ç½‘ç»œè¾“å‡ºæ›´æ¥è¿‘æ ‡ç­¾

æŸå¤±å‡½æ•°ï¼šè¡¡é‡æ¨¡å‹è¾“å‡ºä¸çœŸå®æ ‡ç­¾çš„å·®å¼‚ï¼Œ$Loss=f(\hat{y}, y)$

åå‘ä¼ æ’­åŸç†ï¼šå¾®ç§¯åˆ†ä¸­çš„é“¾å¼æ±‚å¯¼æ³•åˆ™
$$
\mathrm{y}=\mathrm{f}(\mathrm{u}), \mathrm{u}=\mathrm{g}(\mathrm{x}) \quad \frac{\partial \mathrm{y}}{\partial \mathrm{x}}=\frac{\partial \mathrm{y}}{\partial \mathrm{u}} \frac{\partial \mathrm{u}}{\partial \mathrm{x}}
$$


![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230529132846315.png)

å…¶ä¸­ï¼Œ$\phi(Z)$ ä»£è¡¨æ¿€æ´»å‡½æ•°ã€‚$L$ä»£è¡¨æŸå¤±å‡½æ•°ã€‚


$$
\begin{array}{l}
\frac{\partial L}{\partial W^{(2)}}=\operatorname{prod}\left(\frac{\partial L}{\partial O}, \frac{\partial O}{\partial W^{(2)}}\right)=\frac{\partial L}{\partial O} \cdot h^{\top} \\
\frac{\partial L}{\partial h}=\operatorname{prod}\left(\frac{\partial L}{\partial O}, \frac{\partial O}{\partial h}\right)=W^{(2)\top} \cdot \frac{\partial L}{\partial O} \\
\frac{\partial L}{\partial z}=\operatorname{prod}\left(\frac{\partial L}{\partial O}, \frac{\partial O}{\partial h}, \frac{\partial h}{\partial z}\right)=\frac{\partial L}{\partial h} \odot \phi^{\prime}(Z) \\
\frac{\partial L}{\partial W^{(1)}}=\operatorname{prod}\left(\frac{\partial L}{\partial O}, \frac{\partial O}{\partial h}, \frac{\partial h}{\partial z}, \frac{\partial z}{\partial W^{(1)}}\right)=\frac{\partial L}{\partial z} \cdot X^{\top}
\end{array}
$$
å…¶ä¸­ï¼Œprod(x,y) è¡¨ç¤ºxä¸yæ ¹æ®å½¢çŠ¶åšå¿…è¦çš„äº¤æ¢ï¼Œç„¶åç›¸ä¹˜ã€‚$\odot$è¡¨ç¤ºé€å…ƒç´ ç›¸ä¹˜ã€‚é€šè¿‡ä¸Šè¿°å¼å­ï¼Œå¯ä»¥å®¹æ˜“å‘ç°ï¼Œå­˜åœ¨é“¾å¼å…³ç³»ï¼Œå³åä¸€é¡¹çš„æ¢¯åº¦å¯ä»¥ç”¨äºå‰ä¸€é¡¹çš„æ¢¯åº¦ä¸Šã€‚

**ä»Losså‡ºå‘æœ‰å¤šå°‘æ¡é€šè·¯ï¼Œæ¢¯åº¦å°±æœ‰å¤šå°‘é¡¹ï¼Œè¿›è¡Œç›¸åŠ ã€‚ä¸Šå›¾æ‰€ç¤ºï¼Œåªæœ‰ä¸€æ¡é€šè·¯ã€‚ç”¨è“ç¬”ç”»çš„åœ†åœˆï¼Œåˆ™æ˜¯ä¸¤æ¡é€šè·¯ã€‚**

### æ¢¯åº¦ä¸‹é™æ³•ï¼ˆGradient Decentï¼‰

**æ¢¯åº¦ä¸‹é™æ³•**ï¼ˆGradient Decentï¼‰ï¼šæƒå€¼æ²¿æ¢¯åº¦**è´Ÿæ–¹å‘æ›´æ–°**ï¼Œä½¿å‡½æ•°å€¼å‡å°

å¯¼æ•°ï¼šå‡½æ•°åœ¨**æŒ‡å®šåæ ‡è½´ä¸Š**çš„å˜åŒ–ç‡

æ–¹å‘å¯¼æ•°ï¼šæŒ‡å®šæ–¹å‘ä¸Šçš„å˜åŒ–ç‡ï¼ˆåœ¨å¤šç»´ç©ºé—´ï¼‰

æ¢¯åº¦ï¼šä¸€ä¸ªå‘é‡ï¼Œæ–¹å‘ä¸ºæ–¹å‘å¯¼æ•°å–å¾—æœ€å¤§å€¼çš„æ–¹å‘ã€‚

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230529133827958.png)

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230529133853457.png)

### å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰

**å­¦ä¹ ç‡**ï¼ˆLearning Rateï¼‰ï¼šæ§åˆ¶æ›´æ–°æ­¥é•¿

æ²¿æ¢¯åº¦**è´Ÿæ–¹å‘æ›´æ–°**ï¼š
$$
\boldsymbol{w}_{\mathrm{i}+1}=\boldsymbol{w}_{\mathrm{i}}-\boldsymbol{g}\left(\boldsymbol{w}_{\mathrm{i}}\right)
$$
ä¾‹ï¼š
$$
\begin{array}{l}

\mathrm{y}=\mathrm{f}(\mathrm{x})=4 * \mathrm{x}^{2} \\
\mathrm{y}^{\prime}=\mathrm{f}^{\prime}(\mathrm{x})=8 * \mathrm{x} \\
\mathrm{x}_{0}=2, \quad \mathrm{y}_{0}=16, \mathrm{f}^{\prime}\left(\mathrm{x}_{0}\right)=16 \\
\mathrm{x}_{1}=\mathrm{x}_{0}-\mathrm{f}^{\prime}\left(\mathrm{x}_{0}\right)=2-16=-14 \\
\mathrm{x}_{1}=-14, \quad \mathrm{y}_{1}=784, \mathrm{f}^{\prime}\left(\mathrm{x}_{1}\right)=-112 \\
\mathrm{x}_{2}=\mathrm{x}_{1}-\mathrm{f}^{\prime}\left(\mathrm{x}_{1}\right)=-14+112=98, \quad \mathrm{y}_{2}=38416 \\
\end{array}
$$
å¯¹æ¯”ï¼š

æ— å­¦ä¹ ç‡ï¼š$\boldsymbol{w}_{\mathrm{i}+1}=\boldsymbol{w}_{\mathrm{i}}-\boldsymbol{g}\left(\boldsymbol{w}_{\mathrm{i}}\right)$

æœ‰å­¦ä¹ ç‡ï¼š$\boldsymbol{w}_{\mathrm{i}+1}=\boldsymbol{w}_{\mathrm{i}}- LR * \boldsymbol{g}\left(\boldsymbol{w}_{\mathrm{i}}\right)$

ä¸åŒå­¦ä¹ ç‡çš„å½±å“ï¼šåˆ†åˆ«ä¸º1â€”â€”0.25â€”â€”0.24â€”â€”0.1

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230529134435694.png)

## æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰

æŸå¤±å‡½æ•°ï¼šè¡¡é‡æ¨¡å‹è¾“å‡ºä¸çœŸå®çš„æ ‡ç­¾ä¹‹é—´çš„å·®è·

### æŸå¤±å‡½æ•°(Loss Function)

æè¿°çš„å•æ ·æœ¬çš„å·®å¼‚å€¼
$$
\operatorname{Loss}=\mathrm{f}(\hat{\mathrm{y}}, \mathrm{y})
$$

### ä»£ä»·å‡½æ•° (Cost Function)

æè¿°çš„æ˜¯æ€»ä½“æ ·æ ·æœ¬\æ•´ä¸ªæ•°æ®é›†çš„Lossçš„å¹³å‡å€¼ 
$$
cost=\frac{\mathbf{1}}{\mathrm{N}} \sum_{i}^{N} f\left(y_{i}^{\wedge}, y_{i}\right)
$$
### ç›®æ ‡å‡½æ•°(Objective Function)

æ¨¡å‹è¾“å‡ºä¸æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚+æ­£åˆ™é¡¹(æ§åˆ¶æ¨¡å‹å¤æ‚åº¦é˜²æ­¢è¿‡æ‹Ÿåˆç°è±¡)
$$
Obj= Cost + Regularization Term
$$

### ä¸¤ç§å¸¸è§æŸå¤±å‡½æ•°

#### MSE

MSE (å‡æ–¹è¯¯å·®, Mean Squared Error)ï¼šè¾“å‡ºä¸æ ‡ç­¾ä¹‹å·®çš„å¹³æ–¹çš„å‡å€¼ï¼Œå¸¸åœ¨**å›å½’ä»»åŠ¡**ä¸­ä½¿ç”¨ã€‚è®¡ç®—å…¬å¼ï¼š
$$
\mathrm{MSE}=\frac{\sum_{\mathrm{i}=1}^{\mathrm{n}}\left(\mathrm{y}_{\mathrm{i}}-\mathrm{y}_{\mathrm{i}}^{\mathrm{p}}\right)^{2}}{\mathrm{n}}
$$
å…¬å¼ä¸­pä»£è¡¨predictã€‚

> ä¾‹: label  =(1,2)  pred  =(1.5,1.5) 
> $$
> \mathrm{MSE}=\frac{\left[(1-1.5)^{2}+(2-1.5)^{2}\right]}{2}=0.25
> $$

#### CE

CEï¼ˆCross Entropyï¼Œäº¤å‰ç†µ) ï¼šäº¤å‰ç†µæºè‡ªä¿¡æ¯è®ºï¼Œç”¨äºè¡¡é‡ä¸¤ä¸ª**åˆ†å¸ƒ**çš„å·®å¼‚ï¼Œå¸¸åœ¨**åˆ†ç±»ä»»åŠ¡**ä¸­ä½¿ç”¨ã€‚è®¡ç®—å…¬å¼:
$$
\mathrm{H}(\mathrm{p}, \mathrm{q})=-\sum_{\mathrm{i}=1}^{\mathrm{n}} \mathrm{p}\left(\mathrm{x}_{\mathrm{i}}\right) \log \mathrm{q}\left(\mathrm{x}_{\mathrm{i}}\right)
$$
pæ˜¯æŒ‡çœŸæ˜¯åˆ†å¸ƒï¼Œqæ˜¯æ¨¡å‹çš„åˆ†å¸ƒï¼Œè¯•å›¾ç”¨qå»é€¼è¿‘pã€‚åˆ†å¸ƒä¹‹é—´çš„è·ç¦»æ˜¯æ²¡æœ‰å¯¹åº”å…³ç³»çš„ã€‚åœ¨ç»™å®š p çš„æƒ…å†µä¸‹ï¼Œå¦‚æœ q å’Œ p è¶Šæ¥è¿‘ï¼Œäº¤å‰ç†µè¶Šå°ï¼›å¦‚æœ q å’Œ p è¶Šè¿œï¼Œäº¤å‰ç†µå°±è¶Šå¤§ã€‚

+ **è‡ªä¿¡æ¯**ï¼š$I(x)=âˆ’logP(x)$ , p(x) æ˜¯æŸäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡

+ **ä¿¡æ¯ç†µ**ï¼šæè¿°ä¿¡æ¯çš„ä¸ç¡®å®šåº¦ï¼Œä¿¡æ¯ç†µè¶Šå¤§ä¿¡æ¯è¶Šä¸ç¡®å®šï¼Œä¿¡æ¯ç†µè¶Šå°

  **ä¿¡æ¯ç†µ** = æ‰€æœ‰å¯èƒ½å–å€¼çš„ä¿¡æ¯é‡çš„æœŸæœ›ï¼Œå³

$$
\mathrm{H}(\mathrm{x})=\mathrm{E}_{\mathrm{x}-\mathrm{p}}[\mathrm{I}(\mathrm{x})]=-\mathrm{E}[\log \mathrm{P}(\mathrm{x})]=-\sum_{\mathrm{i}=1}^{\mathrm{N}} \mathrm{p}_{\mathrm{i}} \log \left(\mathrm{p}_{\mathrm{i}}\right)
$$

+ **ç›¸å¯¹ç†µ**ï¼šåˆç§°**K-Læ•£åº¦**ï¼Œ**è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚**ã€‚ç”¨æ¦‚ç‡åˆ†å¸ƒ q æ¥è¿‘ä¼¼ p æ—¶æ‰€é€ æˆçš„ä¿¡æ¯æŸå¤±é‡ï¼KL æ•£åº¦æ˜¯æŒ‰ç…§æ¦‚ç‡åˆ†å¸ƒqçš„æœ€ä¼˜ç¼–ç å¯¹çœŸå®åˆ†å¸ƒä¸ºpçš„ä¿¡æ¯è¿›è¡Œç¼–ç ï¼Œå…¶å¹³å‡ç¼–ç é•¿åº¦ï¼ˆå³äº¤å‰ç†µï¼‰ğ»(p, q) å’Œ p çš„æœ€ä¼˜å¹³å‡ç¼–ç é•¿åº¦ï¼ˆå³ç†µï¼‰ğ»(p) ä¹‹é—´çš„å·®å¼‚ï¼
  $$
  \begin{aligned}
  \mathrm{D}_{\mathrm{KL}}(\mathrm{P} \| \mathrm{Q})=\mathrm{E}_{\mathrm{x} \sim \mathrm{p}}\left[\log \frac{\mathrm{P}(\mathrm{x})}{\mathrm{Q}(\mathrm{x})}\right] & =\mathrm{E}_{\mathrm{x}-\mathrm{p}}[\log \mathrm{P}(\mathrm{x})-\log \mathrm{Q}(\mathrm{x})] \\
  & =\sum_{\mathrm{i}=1}^{\mathrm{N}} \mathrm{P}\left(\mathrm{x}_{\mathrm{i}}\right)\left(\log \mathrm{P}\left(\mathrm{x}_{\mathrm{i}}\right)-\log \mathrm{Q}\left(\mathrm{x}_{\mathrm{i}}\right)\right)
  \end{aligned}
  $$

ç”±ä¸Šå¼å¯çŸ¥ï¼Œ$\mathrm{H}(\mathrm{p}, \mathrm{q})=\mathrm{H}(\mathrm{P})+\mathrm{D}_{-} \mathrm{KL}(\mathrm{P} \| \mathrm{Q})$ ,å³ **äº¤å‰ç†µ = ä¿¡æ¯ç†µ + ç›¸å¯¹ç†µ**ã€‚

åˆå› ä¸ºä¿¡æ¯ç†µæ˜¯ä¸€ä¸ªå…³äºpï¼ˆçœŸå®åˆ†å¸ƒï¼‰çš„å¸¸æ•°ï¼Œå› æ­¤**ä¼˜åŒ–äº¤å‰ç†µç­‰ä»·äºä¼˜åŒ–ç›¸å¯¹ç†µ**ã€‚

> ä¸¾ä¾‹ï¼šè®¡ç®—æŸå¤±å‡½æ•°ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
>
> ![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230626164444697.png)
> $$
> \begin{array}{l}
> \mathrm{H}(\mathrm{p}, \mathrm{q})=-\sum_{\mathrm{i}=1}^{\mathrm{n}} \mathrm{p}\left(\mathrm{x}_{\mathrm{i}}\right) \log \mathrm{q}\left(\mathrm{x}_{\mathrm{i}}\right)\\
> \operatorname{loss}=-\left(0^{\star} \log (0.05)+0^{\star} \log (0.1)+1^{\star} \log (0.7)+0^{\star} \log (0.15)\right)=0.36
> \end{array}
> $$

è¿™é‡Œpredictåº”è¯¥æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ï¼Œæ¨¡å‹è¾“å‡ºå¦‚æœæƒ³åŒ–ä¸ºæ¦‚ç‡çš„å½¢å¼ï¼Œå¯ä»¥é‡‡ç”¨softmaxå‡½æ•°ã€‚

##### softmax

Softmaxå‡½æ•°å¯ä»¥å°†å¤šä¸ªæ ‡é‡æ˜ å°„ä¸ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒã€‚ï¼ˆå°†æ•°æ®å˜æ¢åˆ°ç¬¦åˆæ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ï¼‰

> æ¦‚ç‡æœ‰ä¸¤ä¸ªæ€§è´¨ï¼š
>
> 1. æ¦‚ç‡å€¼æ˜¯éè´Ÿçš„
> 2. æ¦‚ç‡ä¹‹å’Œç­‰äº1

$$
\mathrm{y}_{\mathrm{i}}=\mathrm{S}(\boldsymbol{z})_{\mathrm{i}}=\frac{\mathrm{e}^{\mathrm{z}_{\mathrm{i}}}}{\sum_{\mathrm{j}=1}^{\mathrm{C}} \mathrm{e}^{\mathrm{z}_{\mathrm{j}}}}, \mathrm{i}=1, \ldots, \mathrm{C}
$$

| æ¦‚ç‡           | Softmax                   |
| -------------- | ------------------------- |
| æ¦‚ç‡å€¼æ˜¯éè´Ÿçš„ | å–æŒ‡æ•°ï¼Œå®ç°éè´Ÿ          |
| æ¦‚ç‡ä¹‹å’Œç­‰äº1  | é™¤ä»¥æŒ‡æ•°ä¹‹å’Œï¼Œå®ç°ä¹‹å’Œä¸º1 |

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230626165409071.png)

> æ²¡æœ‰ä¸€ä¸ªé€‚åˆæ‰€æœ‰ä»»åŠ¡çš„æŸå¤±å‡½æ•°ï¼ŒæŸå¤±å‡½æ•°è®¾è®¡ä¼šæ¶‰åŠç®—æ³•ç±»å‹ã€æ±‚å¯¼æ˜¯å¦å®¹æ˜“ã€æ•°æ®ä¸­å¼‚å¸¸å€¼çš„åˆ†å¸ƒç­‰é—®é¢˜ã€‚
>
> æ›´å¤šæŸå¤±å‡½æ•°å¯åˆ°PyTorchç½‘ç«™ï¼šhttps://pytorch.org/docs/stable/nn.html#loss-functions
>
> å‡½æ•°è§£è¯»ï¼š https://zhuanlan.zhihu.com/p/61379965

## æƒå€¼åˆå§‹åŒ–ï¼ˆInitializationï¼‰

**æƒå€¼åˆå§‹åŒ–**ï¼šè®­ç»ƒå‰å¯¹æƒå€¼å‚æ•°èµ‹å€¼ï¼Œè‰¯å¥½çš„æƒå€¼åˆå§‹åŒ–æœ‰åˆ©äºæ¨¡å‹è®­ç»ƒ

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230626214827685.png)

> ç®€ä¾¿ä½†é”™è¯¯çš„æ–¹æ³•ï¼šåˆå§‹åŒ–ä¸ºå…¨0â€”â€”ä½¿ç½‘ç»œå±‚é€€åŒ–ï¼Œå¤šå°‘ä¸ªç¥ç»å…ƒéƒ½ç­‰äºä¸€ä¸ªç¥ç»å…ƒï¼Œæ²¡æœ‰è®­ç»ƒçš„æ„ä¹‰ã€‚

### éšæœºåˆå§‹åŒ–æ³•

é«˜æ–¯åˆ†å¸ƒéšæœºåˆå§‹åŒ–ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­éšæœºé‡‡æ ·ï¼Œå¯¹æƒé‡è¿›è¡Œèµ‹å€¼ï¼Œæ¯”å¦‚ Nï½ï¼ˆ0ï¼Œ0.01ï¼‰

> ![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230626215004326.png)
>
> 3$\sigma$å‡†åˆ™: æ•°å€¼åˆ†å¸ƒåœ¨ï¼ˆÎ¼-3$\sigma$,Î¼+3$\sigma$)ä¸­çš„æ¦‚ç‡ä¸º99.73%

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230626215027525.png)

æ§åˆ¶æƒå€¼çš„å°ºåº¦ï¼šä¸èƒ½è®©æƒå€¼å¤ªå°ï¼Œå¦‚æœæƒå€¼è¿‡å°ï¼Œç›¸å½“äºç½‘ç»œé€€åŒ–ã€‚ä¾‹å¦‚å¦‚æœæƒå€¼è¿‡å¤§ï¼Œä¼šä½¿å¾—ä¸€äº›å€¼è½å…¥sigmoidå‡½æ•°ä¸­çš„é¥±å’ŒåŒºåŸŸï¼Œé¥±å’ŒåŒºåŸŸä¸­çš„æ¢¯åº¦æ¥è¿‘äº0ï¼Œä½¿æ¢¯åº¦æ¶ˆå¤±ï¼Œä¸åˆ©äºæ¨¡å‹çš„è®­ç»ƒã€‚

### è‡ªé€‚åº”æ ‡å‡†å·®

è‡ªé€‚åº”æ–¹æ³•éšæœºåˆ†å¸ƒä¸­çš„æ ‡å‡†å·®

+ Xavieråˆå§‹åŒ–ï¼šã€ŠUnderstanding the difficulty of training deep feedforward neural networks ã€‹2010
  $$
  \mathrm{U}\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right)
  $$
  aæ˜¯è¾“å…¥ç¥ç»å…ƒçš„ä¸ªæ•°ï¼Œbæ˜¯è¾“å‡ºç¥ç»å…ƒçš„ä¸ªæ•°ã€‚å‡è®¾$-\sqrt\frac{6}{a+b}$ä¸ºAï¼Œ$\sqrt\frac{6}{a+b}$ä¸ºBï¼Œåˆ™å¯çŸ¥$\frac{A+B}{2}=mean=0$ï¼Œ$std=\sqrt\frac{(B-A)^2}{12}$ã€‚

+ Kaimingåˆå§‹åŒ–/MSRAï¼šã€ŠDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificationã€‹2015

## æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰

**Regularization**ï¼šå‡å°**æ–¹å·®**çš„ç­–ç•¥ï¼Œé€šä¿—ç†è§£ä¸º**å‡è½»è¿‡æ‹Ÿåˆ**çš„ç­–ç•¥

è¯¯å·®å¯åˆ†è§£ä¸ºï¼šåå·®ï¼Œæ–¹å·®ä¸å™ªå£°ä¹‹å’Œã€‚å³**è¯¯å·® = åå·® + æ–¹å·® + å™ªå£°**ä¹‹å’Œï¼ˆè¥¿ç“œä¹¦ï¼‰

+ **åå·®**åº¦é‡äº†å­¦ä¹ ç®—æ³•çš„æœŸæœ›é¢„æµ‹ä¸çœŸå®ç»“æœçš„åç¦»ç¨‹åº¦ï¼Œå³åˆ»ç”»äº†å­¦ä¹ ç®—æ³•æœ¬èº«çš„æ‹Ÿåˆèƒ½åŠ›
+ **æ–¹å·®**åº¦é‡äº†åŒæ ·å¤§å°çš„è®­ç»ƒé›†çš„å˜åŠ¨æ‰€å¯¼è‡´çš„å­¦ä¹ æ€§èƒ½çš„å˜åŒ–ï¼Œå³åˆ»ç”»äº†æ•°æ®æ‰°åŠ¨æ‰€é€ æˆçš„å½±å“
+ **å™ªå£°**åˆ™è¡¨è¾¾äº†åœ¨å½“å‰ä»»åŠ¡ä¸Šä»»ä½•å­¦ä¹ ç®—æ³•æ‰€èƒ½è¾¾åˆ°çš„æœŸæœ›æ³›åŒ–è¯¯å·®çš„ä¸‹ç•Œ

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230626233115981.png)

**è¿‡æ‹Ÿåˆç°è±¡**ï¼šæ–¹å·®è¿‡å¤§ï¼›åœ¨è®­ç»ƒé›†è¡¨ç°è‰¯å¥½ï¼Œåœ¨æµ‹è¯•é›†è¡¨ç°ç³Ÿç³•ã€‚

æŸå¤±å‡½æ•°(Loss Function)ï¼šæè¿°çš„å•æ ·æœ¬çš„å·®å¼‚å€¼
$$
\text { Loss }=\mathrm{f}(\hat{\mathrm{y}}, \mathrm{y})
$$
ä»£ä»·å‡½æ•° (Cost Function)ï¼šæè¿°çš„æ˜¯æ€»ä½“æ ·æ ·æœ¬\æ•´ä¸ªæ•°æ®é›†çš„Lossçš„å¹³å‡å€¼
$$
\text { Cost }=\frac{1}{N} \sum_{i}^{N} \mathrm{f}\left({\hat{y_{i}}}, \mathrm{y}_{\mathrm{i}}\right)
$$
ç›®æ ‡å‡½æ•°(Objective Function)ï¼šæ¨¡å‹è¾“å‡ºä¸æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚+æ­£åˆ™é¡¹ï¼ˆçº¦æŸï¼‰æ§åˆ¶æ¨¡å‹å¤æ‚åº¦é˜²æ­¢è¿‡æ‹Ÿåˆç°è±¡ã€‚

$Obj= Cost + Regularization Term$

L1 Regularization Term:  $\sum_{\mathrm{i}}^{\mathrm{N}}|\mathrm{w}_{\mathrm{i}}|$

L2 Regularization Term:  $\sum_{\mathrm{i}}^{\mathrm{N}} \mathrm{w}_{\mathrm{i}}^{2} $

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230626234254641.png)

ä¸¤ä¸ªæƒå€¼w1å’Œw2ç»„æˆçš„æƒå€¼ç­‰é«˜çº¿ï¼Œåœ¨æ‰€æœ‰ç­‰é«˜çº¿ä¸­é€‰æ‹©L2æ­£åˆ™åŒ–æœ€å°çš„ï¼ˆå³åœ†ä¸æ¤­åœ†ç›¸åˆ‡ï¼‰ï¼ŒL1æ­£åˆ™åŒ–æœ€å°çš„ï¼ˆå³æ–¹æ¡†ä¸æ¤­åœ†ç›¸åˆ‡ï¼‰ã€‚

> å‚è€ƒæ·±åº¦å­¦ä¹ èŠ±ä¹¦ ç¬¬ä¸ƒç« 

### L2 Regularization

**weight decay** (æƒå€¼è¡°å‡)L2æ­£åˆ™åŒ–å«æœ‰æƒå€¼è¡°å‡çš„ä½œç”¨ã€‚

ç›®æ ‡å‡½æ•°(Objective Function):
$$
\begin{array}{l}
\boldsymbol{O} \boldsymbol{b} \boldsymbol{j}=\text { textCost }+ \text { Regularization Term } \\
\boldsymbol{O} \boldsymbol{b} \boldsymbol{j}=\boldsymbol{L} \boldsymbol{o s s}+\frac{\lambda}{2} * \sum_{\mathrm{i}}^{\mathrm{N}} \boldsymbol{w}_{\mathrm{i}}^{2}
\end{array}
$$
æ— æ­£åˆ™é¡¹ï¼š
$$
\mathrm{w}_{\mathrm{i}+1}=\mathrm{w}_{\mathrm{i}}-\frac{\partial \mathrm{obj}}{\partial \mathrm{w}_{\mathrm{i}}}=\mathrm{w}_{\mathrm{i}}-\frac{\partial \text { Loss }}{\partial \mathrm{w}_{\mathrm{i}}}
$$
æœ‰æ­£åˆ™é¡¹: 
$$
\begin{aligned}
\mathrm{w}_{\mathrm{i}+1}=\mathrm{w}_{\mathrm{i}}-\frac{\partial \mathrm{obj}}{\partial \mathrm{w}_{\mathrm{i}}}=\mathrm{w}_{\mathrm{i}} & -\left(\frac{\partial \mathrm{Loss}}{\partial \mathrm{w}_{\mathrm{i}}}+\lambda^{*} \mathrm{w}_{\mathrm{i}}\right) \\
& =\mathrm{w}_{\mathrm{i}}(1-\lambda)-\frac{\partial \mathrm{Loss}}{\partial \mathrm{w}_{\mathrm{i}}}
\end{aligned}
$$
è¿™é‡Œ$\lambda$ç›¸å½“äºä¸€ä¸ªç³»æ•°, $0<\lambda<1$ï¼Œçœ‹æ›´å…³æ³¨Lossè¿˜æ˜¯æ›´å…³æ³¨æ­£åˆ™é¡¹çš„ä¸€ä¸ªè°ƒèŠ‚ç³»æ•°ã€‚é€šè¿‡æœ‰æ— æ­£åˆ™é¡¹çš„å¯¹æ¯”ï¼Œå¯ä»¥çœ‹å‡ºL2æ­£åˆ™åŒ–ç¡®å®èµ·åˆ°äº†è¡°å‡æƒå€¼çš„ä½œç”¨ã€‚

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230626235108454.png)

### Dropout

**Dropout**ï¼šéšæœºå¤±æ´»

**ä¼˜ç‚¹**ï¼šé¿å…è¿‡åº¦ä¾èµ–æŸä¸ªç¥ç»å…ƒï¼Œå®ç°å‡è½»è¿‡æ‹Ÿåˆ

**éšæœº**ï¼šdropout probability ï¼ˆegï¼šp=0.5ï¼‰

**å¤±æ´»**ï¼šweight = 0

![](https://raw.githubusercontent.com/timerring/scratchpad2023/main/2023/image-20230626235633123.png)

æ³¨æ„äº‹é¡¹ï¼šè®­ç»ƒå’Œæµ‹è¯•ä¸¤ä¸ªé˜¶æ®µçš„**æ•°æ®å°ºåº¦å˜åŒ–**ï¼Œä¾‹å¦‚æµ‹è¯•çš„æ—¶å€™ä¼šä½¿ç”¨å»å…¨éƒ¨çš„ç¥ç»å…ƒï¼Œè€Œåœ¨è®­ç»ƒçš„æ—¶å€™ä¼šdropoutä¸€éƒ¨åˆ†ç¥ç»å…ƒã€‚

æµ‹è¯•æ—¶ï¼Œç¥ç»å…ƒè¾“å‡ºå€¼éœ€è¦ä¹˜ä»¥Pï¼ˆæ¦‚ç‡ï¼šä¸€å®šæ¦‚ç‡ä¸¢å¤±ç¥ç»å…ƒï¼‰

> ä¾‹å¦‚ï¼š
>
> æµ‹è¯•æ—¶æ•°æ®è§„æ¨¡æ˜¯100ï¼š$\sum_{i=1}^{100} W_{i} \cdot x_{i}=100 \times P_{p}=50$ è¿™æ—¶éœ€è¦ä¹˜ä¸€ä¸ªéšå³å¤±æ´»çš„æ¦‚ç‡$P_{p}$ï¼Œä½¿æ•°æ®è§„æ¨¡ä¸è®­ç»ƒæ—¶ä¸€è‡´ã€‚
>
> è®­ç»ƒæ—¶æ•°æ®è§„æ¨¡æ˜¯50ï¼š$\sum_{i=1}^{50} W_{i} \cdot x_{i}=100 \times 0.5=50$ è¿™é‡Œæœ‰50%æ¦‚ç‡$P_{p}$ dropoutæ¯ä¸ªç¥ç»å…ƒã€‚

å…¶ä»–æ­£åˆ™åŒ–æ–¹æ³•ï¼šBN, LN, IN, GN.

+ Batch normalizationï¼š ã€ŠBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shiftã€‹
+ Layer Normalizationï¼š ã€ŠLayer Normalizationã€‹
+ Instance Normalizationï¼š ã€ŠInstance Normalization: The Missing Ingredient for Fast Stylizationã€‹
+ Group Normalizationï¼šã€ŠGroup Normalizationã€‹
